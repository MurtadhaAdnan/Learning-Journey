# نظرة عامة على نموذج PPO ودالة الهدف

## 1. نظرة عامة على نموذج PPO

تحسين السياسة القريبة (PPO) هو طريقة "on-policy"، مما يعني أنها تقوم بتحديث السياسة باستخدام البيانات التي تم جمعها من السياسة الحالية. الابتكار الرئيسي في PPO هو استخدام دالة هدف "مقصوصة" للحد من حجم تحديثات السياسة واستقرار التدريب، مما يمنع التحديثات الكبيرة وغير المستقرة.

### الهدف من PPO هو تعظيم العائد المتوقع مع الحفاظ على التحديثات ضمن منطقة آمنة لتجنب التغييرات الكبيرة في السياسة.

### هندسة الممثل-الناقد (Actor-Critic Architecture)

**الممثل (Actor)**
يمثل هذا الجزء من الشبكة السياسة ويخرج الإجراءات بناءً على الحالة.

**الناقد (Critic)**
يقوم هذا الجزء بتقدير دالة قيمة الحالة، ويوفر ملاحظات للممثل حول مدى جودة الحالة الحالية.

## 2. دالة الهدف في PPO

الفكرة الأساسية وراء PPO هي استخدام دالة هدف بديلة تقارب العائد المتوقع للسياسة، مع آلية "قص" لمنع التحديثات الكبيرة بشكل مفرط.

الشكل العام لدالة الهدف في PPO:

$$L_{\text{clip}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]$$

حيث:

* $\theta$ هي معلمات السياسة.
* $r_t(\theta)$ هي نسبة الاحتمالية بين السياسة الجديدة والسياسة القديمة للإجراء $a_t$ المتخذ في الخطوة الزمنية $t$:

    $$r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}$$

* $\hat{A}_t$ هو تقدير الميزة في الخطوة الزمنية $t$، والذي يخبرنا بمدى جودة (أو سوء) الإجراء المتخذ مقارنة بالمتوسط.
* $\epsilon$ هي معلمة قص صغيرة (عادةً ما تكون بين 0.1 و 0.2)، والتي تضمن أن التحديثات لا تغير السياسة بشكل جذري.

* تحتوي دالة الهدف على مكونين:

    **Unclipped Surrogate:** $r_t(\theta) \hat{A}_t$
    يشجع السياسة على زيادة احتمالية الإجراءات الجيدة حيث تكون الميزة موجبة.

    **Clipped Surrogate:** $\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t$
    يمنع النسبة من الزيادة إلى ما بعد حد معين مما يضمن الاستقرار.

## 3. خسارة القيمة (Value Loss)

بالإضافة إلى خسارة السياسة، يتضمن PPO أيضًا خسارة للقيمة لتحسين تقدير وظيفة قيمة الحالة للنقاد (critic). خسارة القيمة هي متوسط الخطأ التربيعي بين القيمة المتنبأ بها للحالة والعائد الحقيقي (القيمة المستهدفة):

$$L_{\text{value}}(\theta) = \mathbb{E}_t \left[ (V_\theta(s_t) - R_t)^2 \right]$$

حيث:

* $V_\theta(s_t)$ هي القيمة المتنبأ بها بواسطة النقاد (دالة قيمة الحالة).
* $R_t$ هو العائد الفعلي (القيمة المستهدفة) للحالة في الوقت $t$، ويتم حسابه غالبًا باستخدام التمهيد (bootstrapping) أو تقدير مونت كارلو (Monte Carlo estimation).

## 4. تسوية الانتروبيا (الاستكشاف)

للتشجيع على الاستكشاف أثناء التدريب، يتضمن PPO حد خسارة الانتروبيا، والذي يثني السياسة عن أن تصبح حتمية مبكرًا جدًا. تشير الانتروبيا العالية إلى المزيد من الاستكشاف، وتشير الانتروبيا المنخفضة إلى أن السياسة تستغل الاستراتيجيات المتعلمة دون استكشاف إجراءات جديدة.

يُعطى حد الانتروبيا بواسطة:

$$L_{\text{entropy}}(\theta) = -\mathbb{E}_t \left[ \pi_\theta(a_t | s_t) \log \pi_\theta(a_t | s_t) \right]$$

حيث:

* $\pi_\theta(a_t | s_t)$ هو توزيع الاحتمالية على الإجراءات في ظل السياسة الحالية.

احتمالية السجل السالب تشجع الاستكشاف عن طريق معاقبة السياسات الحتمية.

## 4. دالة الخسارة الكلية

تجمع دالة الخسارة الكلية لـ PPO بين خسارة السياسة، خسارة القيمة، وخسارة الانتروبيا:

$$L_{\text{PPO}} = L_{\text{clip}}(\theta) + c_1 L_{\text{value}}(\theta) - c_2 L_{\text{entropy}}(\theta)$$

حيث:

* $c_1$ و $c_2$ هي معلمات فائقة تتحكم في الوزن النسبي لخسارة القيمة وخسارة الانتروبيا.

خسارة السياسة $L_{\text{clip}}(\theta)$ هي الحد الأساسي، بينما تساعد خسائر القيمة والانتروبيا على استقرار التدريب وتشجيع الاستكشاف.